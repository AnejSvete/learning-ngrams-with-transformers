import pickle
import random
from collections import defaultdict
from itertools import product
from typing import Dict, List, Optional, Sequence, Union

import numpy as np
from tqdm import trange


class NgramLM:
    def __init__(
        self,
        n: Optional[int] = None,
        alphabet: Optional[str] = None,
        p: Optional[Dict[Sequence[str], Dict[str, float]]] = None,
        alpha: Union[Sequence[float], float] = 0.1,
        mean_length: int = 50,
        binary: bool = False,
        connectivity: float = 0.5,
        filename: Optional[str] = None,
        seed: Optional[int] = None,
    ):
        """Initializes an n-gram language model.

        Args:
            n (Optional[int], optional): The order of the n-gram LM. Defaults to None.
            alphabet (Optional[str], optional): The alphabet of the LM.
                Defaults to None.
            p (Optional[Dict[Sequence[str], Dict[str, float]]], optional): The optional
                existing dictionary of the conditional distributions
                (if initializing from an existing model). Defaults to None.
            alpha (float, optional): The alpha parameter of the Dirichlet distribution.
                Can be a scalar or a list of alphas for each symbol in the alphabet.
                If a list, the number of alphas must be equal to the number of symbols
                + 1 (for the end-of-sentence symbol).
                If a scalar, the same alpha is used for all symbols.
                Defaults to 0.2.
            mean_length (int, optional): The mean length of the strings generated by the
                model. Defaults to 50.
            binary (bool, optional): Whether to produce a random acceptor n-gram model.
                Defaults to False.
            connectivity (Optional[float], optional): The connectivity of the random
                acceptor n-gram model. Defaults to 0.5.
            filename (Optional[str], optional): Filename of an existing model to read.
                Defaults to None.
            seed (Optional[int], optional): The seed of the random generation.
                Defaults to None.
        """
        if seed is not None:
            random.seed(seed)

        if filename is not None:
            self.load(filename)
            self.n = len(list(self.p.keys())[0]) + 1
            self.alphabet = list(self.p[list(self.p.keys())[0]].keys())[:-1]
            self.binary = False
        elif p is not None:
            self.p = p
            self.n = len(list(p.keys())[0]) + 1
            self.alphabet = list(p[list(p.keys())[0]].keys())[:-1]
            self.binary = False
        else:
            self.n = n
            self.alphabet = alphabet
            if isinstance(alpha, float):
                self.alphas = [alpha] * (len(self.alphabet) + 1)
            else:
                self.alphas = alpha
            self.mean_length = mean_length
            self.binary = binary
            self.connectivity = connectivity
            self.p = defaultdict(lambda: defaultdict(float))
            self.rng = np.random.default_rng(seed)

        self.EOSalphabet = list(set(self.alphabet + ["</s>"]))
        self.BOSalphabet = list(set(self.alphabet + ["<s>"]))
        self.BOSEOSalphabet = list(set(self.alphabet + ["<s>", "</s>"]))
        if filename is None and p is None:
            self.setup()

    def _set_next_symbol_probabilities(self, ngram: Sequence[str]) -> List[float]:

        if not self.binary:  # A random n-gram language model
            p = self.rng.dirichlet(self.alphas, size=1)[0]
            p[self.out_sym2idx["</s>"]] = 0
            p = p / np.sum(p)
            p[self.out_sym2idx["</s>"]] = 1 / (self.mean_length - 1)
            p = p / (1 + 1 / (self.mean_length - 1))
        else:  # A random acceptor n-gram model
            p = self.rng.choice(
                [0, 1],
                len(self.EOSalphabet),
                p=[1 - self.connectivity, self.connectivity],
            )
            p_eos = self.rng.choice(
                [0, 1],
                1,
                p=[1 - self.connectivity, self.connectivity],
            )
            p = np.append(p, p_eos)

        for ii, a in self.out_idx2sym.items():
            self.p[ngram][a] = p[ii]

    def setup(self):

        self.sym2idx = {a: ii for ii, a in enumerate(self.BOSEOSalphabet)}
        self.idx2sym = {ii: a for a, ii in self.sym2idx.items()}
        self.in_sym2idx = {a: ii for ii, a in enumerate(self.BOSalphabet)}
        self.in_idx2sym = {ii: a for a, ii in self.in_sym2idx.items()}
        self.out_sym2idx = {a: ii for ii, a in enumerate(self.EOSalphabet)}
        self.out_idx2sym = {ii: a for a, ii in self.out_sym2idx.items()}

        # pre-pad with <s>
        ngram = tuple(["<s>"] * (self.n - 1))
        self._set_next_symbol_probabilities(ngram)

        # pre-pad with <s>
        for ll in range(self.n - 1, 0, -1):
            for ngr in product(self.alphabet, repeat=ll):
                ngram = tuple(["<s>"] * (self.n - ll - 1) + list(ngr))
                self._set_next_symbol_probabilities(ngram)

        # loop over all possible n-1 grams
        for ngram in product(self.alphabet, repeat=self.n - 1):
            self._set_next_symbol_probabilities(ngram)

    def to_dict(self):
        return self.p

    def logprob(self, y: Union[str, List[str]]) -> float:
        """Evaluates the log probability of a single string.

        Args:
            y (Union[str, List[str]]): The string to evaluate.

        Returns:
            float: The log probability of the string.
        """
        ngram = ("<s>",) * (self.n - 1)

        logprob = 0
        for a in y:
            logprob += np.log2(self.p[ngram][a])
            ngram = ngram[1:] + (a,)

        logprob += np.log2(self.p[ngram]["</s>"])

        return logprob

    def accept(self, y: Union[str, List[str]]) -> bool:
        ngram = ("<s>",) * (self.n - 1)

        for a in y:
            if self.p[ngram][a] == 0:
                return False
            ngram = ngram[1:] + (a,)

        return self.p[ngram]["</s>"] > 0

    def _sample_lm(self, to_string: bool = False) -> str:
        ngram = tuple(["<s>"] * (self.n - 1))
        string = list()
        while True:
            p = [self.p[ngram][a] for a in self.EOSalphabet]
            a = random.choices(self.EOSalphabet, p)[0]
            if a == "</s>":
                break
            string.append(a)
            ngram = tuple(list(ngram[1:]) + [a])
        return " ".join(string) if to_string else string

    def _sample_binary(self, to_string: bool = False) -> str:
        string_found = False
        while not string_found:
            ngram = tuple(["<s>"] * (self.n - 1))
            string = list()
            while True:
                p = np.asarray(
                    [self.p[ngram][a] for a in self.EOSalphabet],
                    dtype=np.float64,
                )
                if np.sum(p) == 0:
                    break
                p /= np.sum(p)
                a = random.choices(self.EOSalphabet, p)[0]
                if a == "</s>":
                    string_found = True
                    break
                string.append(a)
                ngram = tuple(list(ngram[1:]) + [a])
        return " ".join(string) if to_string else string

    def sample(self, N: int = 1, to_string: bool = False) -> List[str]:
        if self.binary:
            return [self._sample_binary(to_string) for _ in trange(N)]
        else:
            return [self._sample_lm(to_string) for _ in trange(N)]

    def complement(self) -> "NgramLM":
        assert self.binary, "Complement is only defined for binary models."
        p_complement = defaultdict(lambda: defaultdict(float))
        for ngram, probs in self.p.items():
            p_complement[ngram] = {a: 1 - w for a, w in probs.items()}
        pLMc = NgramLM(p=p_complement)
        pLMc.binary = True
        pLMc.connectivity = self.connectivity
        return pLMc

    def expected_length(self) -> float:
        assert not self.binary, "Expected length is only defined for LM models."
        from art_ngrams.evaluation import metrics

        return metrics.expected_length(
            self.to_dict(), set([a for a in self.alphabet if a != "</s>"])
        )

    def entropy(self) -> float:
        assert not self.binary, "Entropy is only defined for LM models."
        from art_ngrams.evaluation import metrics

        return metrics.entropy(
            self.to_dict(), set([a for a in self.alphabet if a != "</s>"])
        )

    def cross_entropy(self, other: "NgramLM") -> float:
        assert not self.binary, "Cross entropy is only defined for LM models."
        from art_ngrams.evaluation import metrics

        return metrics.cross_entropy(
            self.to_dict(),
            other.to_dict(),
            set([a for a in self.alphabet if a != "</s>"]),
        )

    def kl_divergence(self, other: "NgramLM") -> float:
        assert not self.binary, "KL divergence is only defined for LM models."
        from art_ngrams.evaluation import metrics

        return metrics.kl_divergence(
            self.to_dict(),
            other.to_dict(),
            set([a for a in self.alphabet if a != "</s>"]),
        )

    def save(self, filename: str):

        data = {
            "n": self.n,
            "alphabet": self.alphabet,
            "mean_length": self.mean_length,
            "in_sym2idx": self.in_sym2idx,
            "in_idx2sym": self.in_idx2sym,
            "out_sym2idx": self.out_sym2idx,
            "out_idx2sym": self.out_idx2sym,
            "idx2sym": self.idx2sym,
            "sym2idx": self.sym2idx,
            "EOSalphabet": self.alphabet,
            "BOSalphabet": self.alphabet,
            # "p": self.p,
        }

        with open(filename, "wb") as f:
            pickle.dump(data, f)

    def load(self, filename: str):
        with open(filename, "rb") as f:
            self.p = defaultdict(lambda: defaultdict(float), pickle.load(f))
